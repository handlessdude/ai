{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лаб-4. Рекомендации для коротких сессий"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T00:18:16.118969Z",
     "start_time": "2024-12-23T00:18:16.117096Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ],
   "outputs": [],
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T00:18:16.185265Z",
     "start_time": "2024-12-23T00:18:16.159310Z"
    }
   },
   "source": [
    "IS_CUDA_USED = False\n",
    "device = \"cuda\" if torch.cuda.is_available() and IS_CUDA_USED else \"cpu\"\n",
    "print(f'Device: {device}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве датасета будем использовать архив из мудла.\n",
    "\n",
    "Это немного предобработанная версия [eCommerce events history](https://www.kaggle.com/datasets/mkechinov/ecommerce-events-history-in-electronics-store)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T00:18:16.320385Z",
     "start_time": "2024-12-23T00:18:16.186199Z"
    }
   },
   "source": [
    "# Как и в предыдущей лабораторной пишем собственный загрузчик датасета\n",
    "class ECommerceDataset:\n",
    "    def __init__(self, path):\n",
    "        self.train_data = pd.read_csv(rf\"{path}/train_data.csv\")\n",
    "        self.test_data = pd.read_csv(rf\"{path}/test_data.csv\")\n",
    "\n",
    "        # Добавляем колонку с идентификаторами товаров (для эмбедингов)\n",
    "        all_data = pd.concat([self.train_data, self.test_data])\n",
    "        unique_items = all_data['product_id'].unique()\n",
    "        item_to_idx = pd.Series(data=np.arange(len(unique_items)), index=unique_items)\n",
    "        item_map = pd.DataFrame({'product_id': unique_items, 'product_index': item_to_idx[unique_items].values})\n",
    "        self.train_data = pd.merge(self.train_data, item_map, on='product_id', how='inner')\n",
    "        self.test_data  = pd.merge(self.test_data,  item_map, on='product_id', how='inner')\n",
    "\n",
    "        # Сортируем датасет так, чтобы все сессии оказались рядом, а клики внутри сессии упорядочились по времени\n",
    "        self.train_data.sort_values(['user_session', 'event_time'], inplace=True)\n",
    "        self.test_data.sort_values(['user_session', 'event_time'], inplace=True)\n",
    "\n",
    "# Загрузка большого датасета может занять некоторое время\n",
    "dataset = ECommerceDataset('./eCommerce')"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T00:18:16.343170Z",
     "start_time": "2024-12-23T00:18:16.321119Z"
    }
   },
   "source": [
    "dataset.train_data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       event_time  product_id user_session  product_index\n",
       "32678  1604329884       80548   003pEktS1X           4865\n",
       "34407  1607580196      630753   00ImhDtWxv           4292\n",
       "21963  1607165660      387956   00xjwy5Rb6              8\n",
       "31665  1607168978      387956   00xjwy5Rb6              8\n",
       "23220  1611391773         738   00zEpCxZUK           1478\n",
       "...           ...         ...          ...            ...\n",
       "14766  1613148682       93765   zzaAzAFcYL           3193\n",
       "15086  1613148695       93765   zzaAzAFcYL           3193\n",
       "32226  1613408761      564777   zzveLpjyyb           1226\n",
       "25067  1613409009      564777   zzveLpjyyb           1226\n",
       "2470   1613409044      564777   zzveLpjyyb           1226\n",
       "\n",
       "[41059 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_time</th>\n",
       "      <th>product_id</th>\n",
       "      <th>user_session</th>\n",
       "      <th>product_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32678</th>\n",
       "      <td>1604329884</td>\n",
       "      <td>80548</td>\n",
       "      <td>003pEktS1X</td>\n",
       "      <td>4865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34407</th>\n",
       "      <td>1607580196</td>\n",
       "      <td>630753</td>\n",
       "      <td>00ImhDtWxv</td>\n",
       "      <td>4292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21963</th>\n",
       "      <td>1607165660</td>\n",
       "      <td>387956</td>\n",
       "      <td>00xjwy5Rb6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31665</th>\n",
       "      <td>1607168978</td>\n",
       "      <td>387956</td>\n",
       "      <td>00xjwy5Rb6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23220</th>\n",
       "      <td>1611391773</td>\n",
       "      <td>738</td>\n",
       "      <td>00zEpCxZUK</td>\n",
       "      <td>1478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14766</th>\n",
       "      <td>1613148682</td>\n",
       "      <td>93765</td>\n",
       "      <td>zzaAzAFcYL</td>\n",
       "      <td>3193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15086</th>\n",
       "      <td>1613148695</td>\n",
       "      <td>93765</td>\n",
       "      <td>zzaAzAFcYL</td>\n",
       "      <td>3193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32226</th>\n",
       "      <td>1613408761</td>\n",
       "      <td>564777</td>\n",
       "      <td>zzveLpjyyb</td>\n",
       "      <td>1226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25067</th>\n",
       "      <td>1613409009</td>\n",
       "      <td>564777</td>\n",
       "      <td>zzveLpjyyb</td>\n",
       "      <td>1226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>1613409044</td>\n",
       "      <td>564777</td>\n",
       "      <td>zzveLpjyyb</td>\n",
       "      <td>1226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41059 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T00:18:16.396316Z",
     "start_time": "2024-12-23T00:18:16.343926Z"
    }
   },
   "source": [
    "print(\n",
    "    'Количество уникальных товаров',\n",
    "    pd.concat([dataset.train_data, dataset.test_data])['product_id'].nunique(),\n",
    "    '=',\n",
    "    pd.concat([dataset.train_data, dataset.test_data])['product_index'].max() + 1\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных товаров 15316 = 15316\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За основу мы возьмём модель GRU4Rec из статьи\n",
    "\n",
    "https://arxiv.org/pdf/1511.06939\n",
    "\n",
    "Для обучения сети последовательными действиями пользователей, необходимо сконструировать минибатчи особым образом\n",
    "\n",
    "![Формирование минибатчей](images/mini-batch-creation.png)\n",
    "\n",
    "Так чтобы в инпуте батча содержался набор кликов пользователя, а в таргете набор следующих кликов из той же сессии"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T00:18:16.516896Z",
     "start_time": "2024-12-23T00:18:16.397091Z"
    }
   },
   "source": [
    "class ECommerceLoader():\n",
    "    def __init__(self, data, batch_size, shuffle=False):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.session_count = data['user_session'].nunique()\n",
    "\n",
    "        # Делаем массив с индексами начала и конца каждой сессии\n",
    "        session_sizes = np.array(data.groupby('user_session').size().cumsum())\n",
    "        self.offsets = np.append([0], session_sizes)\n",
    "\n",
    "    def __iter__(self):\n",
    "        session_order = np.arange(self.session_count)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(session_order)\n",
    "\n",
    "        # Заводим список активных сессий, размером с батч\n",
    "        active_sessions = np.arange(self.batch_size)\n",
    "        next_session = self.batch_size # индекс следующей сессии\n",
    "        start = self.offsets[session_order[active_sessions]]   # индексы начал активных сессий\n",
    "        end = self.offsets[session_order[active_sessions] + 1] # индексы концов активных сессий\n",
    "\n",
    "        closed_mask = list(active_sessions) # список сессий, которые открываются на текущей итерации\n",
    "    \n",
    "        while True:\n",
    "            min_len = (end - start).min() # Количество итераций, которые мы можем пройти, пока не закончится какая-то сессия\n",
    "            idx_target = self.data['product_index'].values[start]\n",
    "\n",
    "            # Итерируем по сессиям до тех пор, пока какая-то не закончится\n",
    "            for i in range(min_len - 1):\n",
    "                idx_input = idx_target\n",
    "                idx_target = self.data['product_index'].values[start + i + 1]\n",
    "                input = torch.LongTensor(idx_input)\n",
    "                target = torch.LongTensor(idx_target)\n",
    "                yield input, target, closed_mask # маску мы будем использовать чтобы обнулять новые сессии\n",
    "                closed_mask = []\n",
    "\n",
    "            start = start + (min_len - 1)\n",
    "\n",
    "            # Пробегаемся по сессиям, которые должны быть завершены\n",
    "            closed_mask = np.arange(len(active_sessions))[(end - start) <= 1]\n",
    "            for idx in closed_mask:\n",
    "                # Если новых сессий нет, просто завершаемся\n",
    "                if next_session >= len(self.offsets) - 1:\n",
    "                    return\n",
    "                # Обновляем значения для новой сессии\n",
    "                active_sessions[idx] = next_session\n",
    "                start[idx] = self.offsets[session_order[next_session]]\n",
    "                end[idx]   = self.offsets[session_order[next_session] + 1]\n",
    "                next_session += 1\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "train_loader = ECommerceLoader(dataset.train_data, batch_size, shuffle=True)\n",
    "test_loader  = ECommerceLoader(dataset.test_data, batch_size)"
   ],
   "outputs": [],
   "execution_count": 69
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сама модель основана на рекуррентной архитектуры, такие сети помимо выходного состояния возвращают ещё и скрытое состояние, которое передаётся в сеть на следующей итерации, позволяя таким образом обрабатывать связанные последовательности данных (такие как текст или действия пользователя).\n",
    "\n",
    "![рекуррентная сеть](images/Recurrent_neural_network_unfold.svg.png)\n",
    "\n",
    "В данном случае используется архитектура GRU (это как LSTM, но ещё лучше)\n",
    "![](images/lstm-gru.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T00:18:16.584154Z",
     "start_time": "2024-12-23T00:18:16.517582Z"
    }
   },
   "source": [
    "class GRU4Rec(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        embedding_size = 64\n",
    "        self.hidden_size = 64\n",
    "        item_size = 15316\n",
    "        \n",
    "        self.num_layers = 1\n",
    "        self.state = torch.zeros([self.num_layers, batch_size, self.hidden_size])\n",
    "        self.embedding = nn.Embedding(item_size, embedding_size)\n",
    "        self.gru = nn.GRU(embedding_size, self.hidden_size, num_layers=self.num_layers, batch_first=True)\n",
    "        self.output_layer = nn.Linear(self.hidden_size, item_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    # Перегрузка to чтобы состояние тоже перевести на девайс\n",
    "    def to(self, device):\n",
    "        self.state = self.state.to(device)\n",
    "        return super().to(device)\n",
    "\n",
    "    # Обнуляем состояние для новых сессий\n",
    "    def update_state(self, mask=None):\n",
    "        self.state.detach_()\n",
    "        if mask is None:\n",
    "            self.state = torch.zeros(\n",
    "                self.num_layers, batch_size, self.hidden_size, device=device\n",
    "            )\n",
    "        else:\n",
    "            self.state[:, mask, :] = 0\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.update_state(mask=None)\n",
    "        v = input.unsqueeze(1)\n",
    "        v = self.embedding(v)\n",
    "        v, self.state = self.gru(v, self.state) # (batch_size, 1, hidden_size)\n",
    "        hidden = v.squeeze(1) # (batch_size, hidden_size)\n",
    "        v = self.dropout(hidden)\n",
    "        v = self.output_layer(v)\n",
    "        return v"
   ],
   "outputs": [],
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T00:18:16.709817Z",
     "start_time": "2024-12-23T00:18:16.584833Z"
    }
   },
   "source": [
    "# Тренировка происходит и тестирование\n",
    "\n",
    "def train_iteration(model, data_loader, loss_function, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    for batch, (x, y, m) in enumerate(data_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # Не забываем обнулить состояние\n",
    "        model.update_state(m)\n",
    "\n",
    "        pred = model(x)\n",
    "        loss = loss_function(pred, y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(x)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}]\")\n",
    "\n",
    "def test(model, data_loader, loss_function):\n",
    "    model.eval()\n",
    "\n",
    "    loss, correct, count = 0, 0 ,0\n",
    "    with torch.no_grad():\n",
    "        for x, y, m in data_loader:\n",
    "            count += 1\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            model.update_state(m)\n",
    "            pred = model(x)\n",
    "            loss += loss_function(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    loss = loss / count\n",
    "    correct /= count * batch_size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {loss:>8f} \\n\")\n",
    "    pass\n",
    "\n",
    "\n",
    "def train(epochs, model, loss_function, optimizer):\n",
    "    for t in range(epochs):\n",
    "        print(f\"== Epoch {t + 1} ==\")\n",
    "        train_iteration(model, train_loader, loss_function, optimizer)\n",
    "        test(model, test_loader, loss_function)"
   ],
   "outputs": [],
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T00:20:26.641861Z",
     "start_time": "2024-12-23T00:18:16.710591Z"
    }
   },
   "source": [
    "LEARNING_RATE = 0.001\n",
    "\n",
    "model = GRU4Rec().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train(10, model, loss, optimizer)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Epoch 1 ==\n",
      "loss: 9.649190  [   10]\n",
      "loss: 8.067137  [10010]\n",
      "Test Error: \n",
      " Accuracy: 2.7%, Avg loss: 10.411714 \n",
      "\n",
      "== Epoch 2 ==\n",
      "loss: 7.837283  [   10]\n",
      "loss: 4.936797  [10010]\n",
      "Test Error: \n",
      " Accuracy: 3.3%, Avg loss: 11.482718 \n",
      "\n",
      "== Epoch 3 ==\n",
      "loss: 4.578692  [   10]\n",
      "loss: 5.623654  [10010]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 12.242878 \n",
      "\n",
      "== Epoch 4 ==\n",
      "loss: 2.898281  [   10]\n",
      "loss: 1.515950  [10010]\n",
      "Test Error: \n",
      " Accuracy: 3.7%, Avg loss: 12.657432 \n",
      "\n",
      "== Epoch 5 ==\n",
      "loss: 4.118772  [   10]\n",
      "loss: 2.973674  [10010]\n",
      "Test Error: \n",
      " Accuracy: 3.8%, Avg loss: 12.899476 \n",
      "\n",
      "== Epoch 6 ==\n",
      "loss: 2.006286  [   10]\n",
      "loss: 2.730523  [10010]\n",
      "Test Error: \n",
      " Accuracy: 3.8%, Avg loss: 13.271358 \n",
      "\n",
      "== Epoch 7 ==\n",
      "loss: 2.514724  [   10]\n",
      "loss: 1.746758  [10010]\n",
      "Test Error: \n",
      " Accuracy: 3.9%, Avg loss: 13.787472 \n",
      "\n",
      "== Epoch 8 ==\n",
      "loss: 3.621876  [   10]\n",
      "loss: 2.073610  [10010]\n",
      "Test Error: \n",
      " Accuracy: 3.9%, Avg loss: 14.359397 \n",
      "\n",
      "== Epoch 9 ==\n",
      "loss: 1.069592  [   10]\n",
      "loss: 2.823932  [10010]\n",
      "Test Error: \n",
      " Accuracy: 4.0%, Avg loss: 14.935636 \n",
      "\n",
      "== Epoch 10 ==\n",
      "loss: 2.696525  [   10]\n",
      "loss: 2.101473  [10010]\n",
      "Test Error: \n",
      " Accuracy: 4.0%, Avg loss: 15.506798 \n",
      "\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задания\n",
    "\n",
    "Основные:\n",
    "- Достичь точности в 3.5% на этом датасете - 5 баллов\n",
    "- На основе GRU4Rec построить модель для датасета из предыдущей лабораторной (Movielens) - 5 баллов\n",
    "\n",
    "Дополнительные задания:\n",
    "- Реализовать одну из функций потерь BPR или TOP1 (https://arxiv.org/pdf/1511.06939) - 5 баллов\n",
    "- Реализовать вторую функцию потерь - 5 баллов\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полезные ссылки\n",
    "\n",
    "Полезные ссылки по рекомендательным системам, модели из лекции и не только\n",
    "\n",
    "- Репозиторий с кучей информации по рекомендательным системам https://github.com/recommenders-team/recommenders\n",
    "- Рекомендательные системы на основе свёрток https://arxiv.org/pdf/1809.07426\n",
    "- Sequence-Aware Factorization Machines (машина факторизации для временных последовательностей) https://arxiv.org/pdf/1911.02752\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
