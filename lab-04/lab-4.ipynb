{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лаб-4. Рекомендации для коротких сессий"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T21:51:07.744576Z",
     "start_time": "2024-12-27T21:51:07.742609Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 125
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T21:51:07.802448Z",
     "start_time": "2024-12-27T21:51:07.772535Z"
    }
   },
   "source": [
    "IS_CUDA_USED = False\n",
    "device = \"cuda\" if torch.cuda.is_available() and IS_CUDA_USED else \"cpu\"\n",
    "print(f'Device: {device}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "execution_count": 126
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T21:51:07.955399Z",
     "start_time": "2024-12-27T21:51:07.803914Z"
    }
   },
   "source": [
    "# Как и в предыдущей лабораторной пишем собственный загрузчик датасета\n",
    "class ECommerceDataset:\n",
    "    def __init__(self, path):\n",
    "        self.train_data = pd.read_csv(rf\"{path}/train_data.csv\")\n",
    "        self.test_data = pd.read_csv(rf\"{path}/test_data.csv\")\n",
    "\n",
    "        # Добавляем колонку с идентификаторами товаров (для эмбедингов)\n",
    "        all_data = pd.concat([self.train_data, self.test_data])\n",
    "        unique_items = all_data['product_id'].unique()\n",
    "        item_to_idx = pd.Series(data=np.arange(len(unique_items)), index=unique_items)\n",
    "        item_map = pd.DataFrame({'product_id': unique_items, 'product_index': item_to_idx[unique_items].values})\n",
    "        self.train_data = pd.merge(self.train_data, item_map, on='product_id', how='inner')\n",
    "        self.test_data  = pd.merge(self.test_data,  item_map, on='product_id', how='inner')\n",
    "\n",
    "        # Сортируем датасет так, чтобы все сессии оказались рядом, а клики внутри сессии упорядочились по времени\n",
    "        self.train_data.sort_values(['user_session', 'event_time'], inplace=True)\n",
    "        self.test_data.sort_values(['user_session', 'event_time'], inplace=True)\n",
    "\n",
    "# Загрузка большого датасета может занять некоторое время\n",
    "dataset = ECommerceDataset('./eCommerce')"
   ],
   "outputs": [],
   "execution_count": 127
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T21:51:07.979907Z",
     "start_time": "2024-12-27T21:51:07.956061Z"
    }
   },
   "source": [
    "dataset.train_data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       event_time  product_id user_session  product_index\n",
       "32678  1604329884       80548   003pEktS1X           4865\n",
       "34407  1607580196      630753   00ImhDtWxv           4292\n",
       "21963  1607165660      387956   00xjwy5Rb6              8\n",
       "31665  1607168978      387956   00xjwy5Rb6              8\n",
       "23220  1611391773         738   00zEpCxZUK           1478\n",
       "...           ...         ...          ...            ...\n",
       "14766  1613148682       93765   zzaAzAFcYL           3193\n",
       "15086  1613148695       93765   zzaAzAFcYL           3193\n",
       "32226  1613408761      564777   zzveLpjyyb           1226\n",
       "25067  1613409009      564777   zzveLpjyyb           1226\n",
       "2470   1613409044      564777   zzveLpjyyb           1226\n",
       "\n",
       "[41059 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_time</th>\n",
       "      <th>product_id</th>\n",
       "      <th>user_session</th>\n",
       "      <th>product_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32678</th>\n",
       "      <td>1604329884</td>\n",
       "      <td>80548</td>\n",
       "      <td>003pEktS1X</td>\n",
       "      <td>4865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34407</th>\n",
       "      <td>1607580196</td>\n",
       "      <td>630753</td>\n",
       "      <td>00ImhDtWxv</td>\n",
       "      <td>4292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21963</th>\n",
       "      <td>1607165660</td>\n",
       "      <td>387956</td>\n",
       "      <td>00xjwy5Rb6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31665</th>\n",
       "      <td>1607168978</td>\n",
       "      <td>387956</td>\n",
       "      <td>00xjwy5Rb6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23220</th>\n",
       "      <td>1611391773</td>\n",
       "      <td>738</td>\n",
       "      <td>00zEpCxZUK</td>\n",
       "      <td>1478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14766</th>\n",
       "      <td>1613148682</td>\n",
       "      <td>93765</td>\n",
       "      <td>zzaAzAFcYL</td>\n",
       "      <td>3193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15086</th>\n",
       "      <td>1613148695</td>\n",
       "      <td>93765</td>\n",
       "      <td>zzaAzAFcYL</td>\n",
       "      <td>3193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32226</th>\n",
       "      <td>1613408761</td>\n",
       "      <td>564777</td>\n",
       "      <td>zzveLpjyyb</td>\n",
       "      <td>1226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25067</th>\n",
       "      <td>1613409009</td>\n",
       "      <td>564777</td>\n",
       "      <td>zzveLpjyyb</td>\n",
       "      <td>1226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>1613409044</td>\n",
       "      <td>564777</td>\n",
       "      <td>zzveLpjyyb</td>\n",
       "      <td>1226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41059 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 128
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T21:51:08.048298Z",
     "start_time": "2024-12-27T21:51:07.980718Z"
    }
   },
   "source": [
    "print(\n",
    "    'Количество уникальных товаров',\n",
    "    pd.concat([dataset.train_data, dataset.test_data])['product_id'].nunique(),\n",
    "    '=',\n",
    "    pd.concat([dataset.train_data, dataset.test_data])['product_index'].max() + 1\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных товаров 15316 = 15316\n"
     ]
    }
   ],
   "execution_count": 129
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T21:51:08.177748Z",
     "start_time": "2024-12-27T21:51:08.048977Z"
    }
   },
   "source": [
    "class ECommerceLoader():\n",
    "    def __init__(self, data, batch_size, shuffle=False):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.session_count = data['user_session'].nunique()\n",
    "\n",
    "        # Делаем массив с индексами начала и конца каждой сессии\n",
    "        session_sizes = np.array(data.groupby('user_session').size().cumsum())\n",
    "        self.offsets = np.append([0], session_sizes)\n",
    "\n",
    "    def __iter__(self):\n",
    "        session_order = np.arange(self.session_count)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(session_order)\n",
    "\n",
    "        # Заводим список активных сессий, размером с батч\n",
    "        active_sessions = np.arange(self.batch_size)\n",
    "        next_session = self.batch_size # индекс следующей сессии\n",
    "        start = self.offsets[session_order[active_sessions]]   # индексы начал активных сессий\n",
    "        end = self.offsets[session_order[active_sessions] + 1] # индексы концов активных сессий\n",
    "\n",
    "        closed_mask = list(active_sessions) # список сессий, которые открываются на текущей итерации\n",
    "    \n",
    "        while True:\n",
    "            min_len = (end - start).min() # Количество итераций, которые мы можем пройти, пока не закончится какая-то сессия\n",
    "            idx_target = self.data['product_index'].values[start]\n",
    "\n",
    "            # Итерируем по сессиям до тех пор, пока какая-то не закончится\n",
    "            for i in range(min_len - 1):\n",
    "                idx_input = idx_target\n",
    "                idx_target = self.data['product_index'].values[start + i + 1]\n",
    "                input = torch.LongTensor(idx_input)\n",
    "                target = torch.LongTensor(idx_target)\n",
    "                yield input, target, closed_mask # маску мы будем использовать чтобы обнулять новые сессии\n",
    "                closed_mask = []\n",
    "\n",
    "            start = start + (min_len - 1)\n",
    "\n",
    "            # Пробегаемся по сессиям, которые должны быть завершены\n",
    "            closed_mask = np.arange(len(active_sessions))[(end - start) <= 1]\n",
    "            for idx in closed_mask:\n",
    "                # Если новых сессий нет, просто завершаемся\n",
    "                if next_session >= len(self.offsets) - 1:\n",
    "                    return\n",
    "                # Обновляем значения для новой сессии\n",
    "                active_sessions[idx] = next_session\n",
    "                start[idx] = self.offsets[session_order[next_session]]\n",
    "                end[idx]   = self.offsets[session_order[next_session] + 1]\n",
    "                next_session += 1\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "train_loader = ECommerceLoader(dataset.train_data, batch_size, shuffle=True)\n",
    "test_loader  = ECommerceLoader(dataset.test_data, batch_size)"
   ],
   "outputs": [],
   "execution_count": 130
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T21:51:08.245854Z",
     "start_time": "2024-12-27T21:51:08.178453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EMBEDDING_SIZE = 64\n",
    "\n",
    "HIDDEN_SIZE = 64\n",
    "\n",
    "ITEM_SIZE = 64\n",
    "\n",
    "class GRU4Rec(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        embedding_size = 64\n",
    "        self.hidden_size = 64\n",
    "        item_size = 15316\n",
    "        \n",
    "        self.num_layers = 1\n",
    "        self.state = torch.zeros([self.num_layers, batch_size, self.hidden_size])\n",
    "        self.embedding = nn.Embedding(item_size, embedding_size)\n",
    "        self.gru = nn.GRU(embedding_size, self.hidden_size, num_layers=self.num_layers, batch_first=True)\n",
    "        self.output_layer = nn.Linear(self.hidden_size, item_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    # Перегрузка to чтобы состояние тоже перевести на девайс\n",
    "    def to(self, device):\n",
    "        self.state = self.state.to(device)\n",
    "        return super().to(device)\n",
    "\n",
    "    # Обнуляем состояние для новых сессий\n",
    "    def update_state(self, mask=None):\n",
    "        self.state.detach_()\n",
    "        if mask is None:\n",
    "            self.state = torch.zeros(\n",
    "                self.num_layers, batch_size, self.hidden_size, device=device\n",
    "            )\n",
    "        else:\n",
    "            self.state[:, mask, :] = 0\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.update_state(mask=None)\n",
    "        v = input.unsqueeze(1)\n",
    "        v = self.embedding(v)\n",
    "        v, self.state = self.gru(v, self.state) # (batch_size, 1, hidden_size)\n",
    "        hidden = v.squeeze(1) # (batch_size, hidden_size)\n",
    "        v = self.dropout(hidden)\n",
    "        v = self.output_layer(v)\n",
    "        return v"
   ],
   "outputs": [],
   "execution_count": 131
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T21:51:08.338308Z",
     "start_time": "2024-12-27T21:51:08.246505Z"
    }
   },
   "source": [
    "# Тренировка происходит и тестирование\n",
    "\n",
    "def train_iteration(model, data_loader, loss_function, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    for batch, (x, y, m) in enumerate(data_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # Не забываем обнулить состояние\n",
    "        model.update_state(m)\n",
    "\n",
    "        pred = model(x)\n",
    "        loss = loss_function(pred, y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(x)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}]\")\n",
    "\n",
    "def test(model, data_loader, loss_function):\n",
    "    model.eval()\n",
    "\n",
    "    loss, correct, count = 0, 0 ,0\n",
    "    with torch.no_grad():\n",
    "        for x, y, m in data_loader:\n",
    "            count += 1\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            model.update_state(m)\n",
    "            pred = model(x)\n",
    "            loss += loss_function(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    loss = loss / count\n",
    "    correct /= count * batch_size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {loss:>8f} \\n\")\n",
    "    pass\n",
    "\n",
    "\n",
    "def train(epochs, model, loss_function, optimizer):\n",
    "    for t in tqdm(range(epochs)):\n",
    "        print(f\"== Epoch {t + 1} ==\")\n",
    "        train_iteration(model, train_loader, loss_function, optimizer)\n",
    "        test(model, test_loader, loss_function)"
   ],
   "outputs": [],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T21:51:08.549473Z",
     "start_time": "2024-12-27T21:51:08.338967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BPRLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Bayesian Personalized Ranking Loss\n",
    "    \"\"\"\n",
    "    def __init__(self, default_num_negatives=10):\n",
    "        super(BPRLoss, self).__init__()\n",
    "        self.default_num_negatives = default_num_negatives\n",
    "        self.num_negatives = default_num_negatives\n",
    "\n",
    "    def forward(self, predictions, ground_truth):\n",
    "        \"\"\"\n",
    "        predictions: [B, N] - predicted scores for all items\n",
    "        ground_truth: [B] - indices of ground truth items for all sessions in a batch\n",
    "        \"\"\"\n",
    "        batch_size, num_items = predictions.size()\n",
    "\n",
    "        self.num_negatives = num_items - 1 if USE_ALL_NEGATIVES else self.default_num_negatives\n",
    "        \n",
    "        positive_scores = predictions[torch.arange(batch_size), ground_truth]\n",
    "\n",
    "        # negatives = torch.randint(0, num_items, (batch_size, self.num_negatives), device=predictions.device)\n",
    "        # negative_scores = predictions.gather(1, negatives)  # [B, num_negatives]\n",
    "        negative_indices = torch.arange(num_items, device=predictions.device).repeat(batch_size, 1) # [0, 1, ..., num_items - 1] x batch_size -> [batch_size, num_items]\n",
    "        negative_indices.scatter_(1, ground_truth.unsqueeze(1), -1)  # Mask positives with -1\n",
    "        negatives = negative_indices[negative_indices != -1].view(batch_size, -1)  # Filter out positives\n",
    "        sampled_negatives = negatives[:, torch.randint(0, negatives.size(1), (self.num_negatives,), device=predictions.device)]\n",
    "        negative_scores = predictions.gather(1, sampled_negatives)  # [B, num_negatives]\n",
    "\n",
    "        diff = positive_scores.unsqueeze(1) - negative_scores  # [B, num_negatives]\n",
    "        loss = -torch.mean(torch.log(torch.sigmoid(diff)))\n",
    "        return loss"
   ],
   "outputs": [],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T21:51:08.922214Z",
     "start_time": "2024-12-27T21:51:08.550456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TOP1Loss(nn.Module):\n",
    "    \"\"\"\n",
    "    TOP1 Loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, default_num_negatives=10):\n",
    "        super(TOP1Loss, self).__init__()\n",
    "        self.default_num_negatives = default_num_negatives\n",
    "        self.num_negatives = default_num_negatives\n",
    "\n",
    "    def forward(self, predictions, ground_truth):\n",
    "        \"\"\"\n",
    "        predictions: [B, N] - predicted scores for all items\n",
    "        ground_truth: [B] - indices of ground truth items\n",
    "        \"\"\"\n",
    "        batch_size, num_items = predictions.size()\n",
    "\n",
    "        self.num_negatives = num_items - 1 if USE_ALL_NEGATIVES else self.default_num_negatives\n",
    "\n",
    "        positive_scores = predictions[torch.arange(batch_size), ground_truth]\n",
    "\n",
    "        # negatives = torch.randint(0, num_items, (batch_size, self.num_negatives), device=predictions.device)\n",
    "        # negative_scores = predictions.gather(1, negatives)  # [B, num_negatives]\n",
    "        negative_indices = torch.arange(num_items, device=predictions.device).repeat(batch_size, 1) # [0, 1, ..., num_items - 1] x batch_size -> [batch_size, num_items]\n",
    "        negative_indices.scatter_(1, ground_truth.unsqueeze(1), -1)  # Mask positives with -1\n",
    "        negatives = negative_indices[negative_indices != -1].view(batch_size, -1)  # Filter out positives\n",
    "        sampled_negatives = negatives[:, torch.randint(0, negatives.size(1), (self.num_negatives,), device=predictions.device)]\n",
    "        negative_scores = predictions.gather(1, sampled_negatives)  # [B, num_negatives]\n",
    "        \n",
    "        rank_term = torch.sigmoid(negative_scores - positive_scores.unsqueeze(1))\n",
    "        regularization_term = torch.sigmoid(negative_scores**2)\n",
    "\n",
    "        loss = torch.mean(rank_term + regularization_term)\n",
    "        return loss"
   ],
   "outputs": [],
   "execution_count": 134
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T21:51:09.013852Z",
     "start_time": "2024-12-27T21:51:08.922975Z"
    }
   },
   "source": [
    "LOSS_FUNCTIONS = [\n",
    "    nn.CrossEntropyLoss,\n",
    "    BPRLoss,\n",
    "    TOP1Loss,\n",
    "]\n",
    "\n",
    "LOSS_FUNCTION_SAMPLE_ENABLED = [1, 1, 1]\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "EPOCHS_COUNT = 10\n",
    "\n",
    "USE_ALL_NEGATIVES = True\n",
    "\n",
    "def launch_loss_example(loss_function_idx):\n",
    "    if not LOSS_FUNCTION_SAMPLE_ENABLED[loss_function_idx]:\n",
    "        return\n",
    "    \n",
    "    loss = LOSS_FUNCTIONS[loss_function_idx]()\n",
    "    print(loss)\n",
    "    \n",
    "    model = GRU4Rec().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    train(EPOCHS_COUNT, model, loss, optimizer)"
   ],
   "outputs": [],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T21:53:33.850061Z",
     "start_time": "2024-12-27T21:51:09.014499Z"
    }
   },
   "cell_type": "code",
   "source": "launch_loss_example(0)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Epoch 1 ==\n",
      "loss: 9.592493  [   10]\n",
      "loss: 8.483533  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:15<02:23, 16.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 3.3%, Avg loss: 11.908197 \n",
      "\n",
      "== Epoch 2 ==\n",
      "loss: 6.140446  [   10]\n",
      "loss: 3.488057  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:29<01:58, 14.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 13.027132 \n",
      "\n",
      "== Epoch 3 ==\n",
      "loss: 3.462886  [   10]\n",
      "loss: 3.179880  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:44<01:42, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 13.911704 \n",
      "\n",
      "== Epoch 4 ==\n",
      "loss: 3.646265  [   10]\n",
      "loss: 4.191558  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:58<01:26, 14.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 3.7%, Avg loss: 14.652882 \n",
      "\n",
      "== Epoch 5 ==\n",
      "loss: 2.622064  [   10]\n",
      "loss: 1.574628  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:13<01:12, 14.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 15.449976 \n",
      "\n",
      "== Epoch 6 ==\n",
      "loss: 1.686979  [   10]\n",
      "loss: 2.137448  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:27<00:57, 14.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 3.8%, Avg loss: 16.179389 \n",
      "\n",
      "== Epoch 7 ==\n",
      "loss: 1.338903  [   10]\n",
      "loss: 3.167351  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:41<00:43, 14.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 3.8%, Avg loss: 16.927399 \n",
      "\n",
      "== Epoch 8 ==\n",
      "loss: 4.060834  [   10]\n",
      "loss: 3.501345  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:56<00:28, 14.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 3.9%, Avg loss: 17.569618 \n",
      "\n",
      "== Epoch 9 ==\n",
      "loss: 3.252516  [   10]\n",
      "loss: 2.807049  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [02:10<00:14, 14.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 3.9%, Avg loss: 18.059037 \n",
      "\n",
      "== Epoch 10 ==\n",
      "loss: 3.280342  [   10]\n",
      "loss: 2.425290  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:24<00:00, 14.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 3.8%, Avg loss: 18.537528 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 136
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T21:56:14.419627Z",
     "start_time": "2024-12-27T21:53:33.850748Z"
    }
   },
   "cell_type": "code",
   "source": "launch_loss_example(1)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPRLoss()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Epoch 1 ==\n",
      "loss: 0.707352  [   10]\n",
      "loss: 0.491144  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:16<02:30, 16.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 2.1%, Avg loss: 1.552267 \n",
      "\n",
      "== Epoch 2 ==\n",
      "loss: 0.093937  [   10]\n",
      "loss: 0.158081  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:33<02:14, 16.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 2.9%, Avg loss: 1.746116 \n",
      "\n",
      "== Epoch 3 ==\n",
      "loss: 0.027332  [   10]\n",
      "loss: 0.022029  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:50<01:57, 16.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 3.2%, Avg loss: 1.721488 \n",
      "\n",
      "== Epoch 4 ==\n",
      "loss: 0.022582  [   10]\n",
      "loss: 0.140375  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:06<01:38, 16.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 3.5%, Avg loss: 1.770576 \n",
      "\n",
      "== Epoch 5 ==\n",
      "loss: 0.020967  [   10]\n",
      "loss: 0.012533  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:22<01:21, 16.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 3.5%, Avg loss: 1.826463 \n",
      "\n",
      "== Epoch 6 ==\n",
      "loss: 0.008798  [   10]\n",
      "loss: 0.000849  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:37<01:04, 16.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 3.5%, Avg loss: 1.805444 \n",
      "\n",
      "== Epoch 7 ==\n",
      "loss: 0.015321  [   10]\n",
      "loss: 0.007916  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:53<00:47, 15.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 1.747688 \n",
      "\n",
      "== Epoch 8 ==\n",
      "loss: 0.008842  [   10]\n",
      "loss: 0.001325  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [02:09<00:31, 15.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 3.7%, Avg loss: 1.717291 \n",
      "\n",
      "== Epoch 9 ==\n",
      "loss: 0.007671  [   10]\n",
      "loss: 0.007106  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [02:24<00:15, 15.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 3.7%, Avg loss: 1.878401 \n",
      "\n",
      "== Epoch 10 ==\n",
      "loss: 0.015361  [   10]\n",
      "loss: 0.000313  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:40<00:00, 16.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 3.8%, Avg loss: 1.921382 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T21:58:56.999340Z",
     "start_time": "2024-12-27T21:56:14.420220Z"
    }
   },
   "cell_type": "code",
   "source": "launch_loss_example(2)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP1Loss()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Epoch 1 ==\n",
      "loss: 1.010114  [   10]\n",
      "loss: 0.733474  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:15<02:21, 15.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 2.3%, Avg loss: 1.029638 \n",
      "\n",
      "== Epoch 2 ==\n",
      "loss: 0.637657  [   10]\n",
      "loss: 0.665506  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:31<02:07, 15.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 1.8%, Avg loss: 1.044967 \n",
      "\n",
      "== Epoch 3 ==\n",
      "loss: 0.589930  [   10]\n",
      "loss: 0.617662  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:47<01:50, 15.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 1.6%, Avg loss: 1.046010 \n",
      "\n",
      "== Epoch 4 ==\n",
      "loss: 0.565684  [   10]\n",
      "loss: 0.566488  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:03<01:35, 15.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 1.4%, Avg loss: 1.045264 \n",
      "\n",
      "== Epoch 5 ==\n",
      "loss: 0.579742  [   10]\n",
      "loss: 0.581812  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:19<01:20, 16.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 1.4%, Avg loss: 1.039070 \n",
      "\n",
      "== Epoch 6 ==\n",
      "loss: 0.575898  [   10]\n",
      "loss: 0.568635  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:35<01:04, 16.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 1.4%, Avg loss: 1.038429 \n",
      "\n",
      "== Epoch 7 ==\n",
      "loss: 0.622132  [   10]\n",
      "loss: 0.572440  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:51<00:48, 16.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 1.3%, Avg loss: 1.039363 \n",
      "\n",
      "== Epoch 8 ==\n",
      "loss: 0.565481  [   10]\n",
      "loss: 0.592677  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [02:08<00:32, 16.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 1.041955 \n",
      "\n",
      "== Epoch 9 ==\n",
      "loss: 0.573973  [   10]\n",
      "loss: 0.567552  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [02:25<00:16, 16.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 1.2%, Avg loss: 1.040649 \n",
      "\n",
      "== Epoch 10 ==\n",
      "loss: 0.566969  [   10]\n",
      "loss: 0.557183  [10010]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:42<00:00, 16.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 1.2%, Avg loss: 1.040725 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 138
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задания\n",
    "\n",
    "Основные:\n",
    "- Достичь точности в 3.5% на этом датасете - 5 баллов\n",
    "- На основе GRU4Rec построить модель для датасета из предыдущей лабораторной (Movielens) - 5 баллов\n",
    "\n",
    "Дополнительные задания:\n",
    "- Реализовать одну из функций потерь BPR или TOP1 (https://arxiv.org/pdf/1511.06939) - 5 баллов\n",
    "- Реализовать вторую функцию потерь - 5 баллов\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полезные ссылки\n",
    "\n",
    "Полезные ссылки по рекомендательным системам, модели из лекции и не только\n",
    "\n",
    "- Репозиторий с кучей информации по рекомендательным системам https://github.com/recommenders-team/recommenders\n",
    "- Рекомендательные системы на основе свёрток https://arxiv.org/pdf/1809.07426\n",
    "- Sequence-Aware Factorization Machines (машина факторизации для временных последовательностей) https://arxiv.org/pdf/1911.02752\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
