{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лаб-3. Рекомендательные системы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбираем девайс\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве датасета будем использовать MovieLens\n",
    "\n",
    "https://grouplens.org/datasets/movielens/\n",
    "\n",
    "А именно, самый маленький вариант со 100 тыс. оценок\n",
    "\n",
    "https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для загрузки датасета напишем свою реализацию класса Dataset\n",
    "class MovielensDataset(Dataset):\n",
    "    r\"\"\"seed должен быть одинаковым для обучающей и тренировочной выборки\"\"\"\n",
    "    def __init__(self, source, train=True, seed=1):\n",
    "        ratings      = pd.read_csv(rf\"{source}\\ratings.csv\")\n",
    "        self.movies  = pd.read_csv(rf\"{source}\\movies.csv\")\n",
    "\n",
    "        # Преобразовываем Id фильмов в индексы в таблице movies\n",
    "        x = self.movies.loc[:,['movieId']]\n",
    "        x['movieId'], x.index = x.index, x['movieId'].values\n",
    "        ratings['movieId'] = ratings['movieId'].map(x.to_dict()['movieId'])\n",
    "\n",
    "        # делим датасет 80% на 20%\n",
    "        train_data = ratings.sample(frac=0.8, random_state=seed)\n",
    "        test_data  = ratings.drop(train_data.index)\n",
    "\n",
    "        self.ratings = train_data if train else test_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.ratings.iloc[idx]\n",
    "        return {\n",
    "            \"user\": torch.LongTensor([sample['userId']]),\n",
    "            \"movie\": torch.LongTensor([sample['movieId']]),\n",
    "            \"rating\": torch.FloatTensor([sample['rating']])\n",
    "        }\n",
    "\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "sataset_source = r'.\\ml-latest-small'\n",
    "\n",
    "movielens_train = MovielensDataset(sataset_source, train=True)\n",
    "movielens_test  = MovielensDataset(sataset_source, train=False)\n",
    "\n",
    "train_loader = DataLoader(movielens_train, batch_size, True)\n",
    "test_loader = DataLoader(movielens_test, batch_size, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    for k, v in batch.items():\n",
    "        print(k, v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функции для обучения из прошлой лабы, с учётом юзеров и айтемов\n",
    "\n",
    "def train_iteration(model, data_loader, loss_function, optimizer):\n",
    "    model.train()\n",
    "    train_size = len(data_loader.dataset)\n",
    "    for idx, batch in enumerate(data_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        pred = model(batch)\n",
    "        loss = loss_function(pred, batch['rating'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if idx % 100 == 0:\n",
    "            loss, current = loss.item(), (idx + 1) * batch_size\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{train_size:>5d}]\")\n",
    "\n",
    "def test(model, data_loader, loss_function):\n",
    "    model.eval()\n",
    "    num_batches = len(data_loader)\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            pred = model(batch)\n",
    "            loss += loss_function(pred, batch['rating']).item()\n",
    "\n",
    "    loss /= num_batches\n",
    "    print(f\"Avg loss: {loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "def train(epochs, model, loss_function, optimizer):\n",
    "    for t in range(epochs):\n",
    "        print(f\"== Epoch {t + 1} ==\")\n",
    "        train_iteration(model, train_loader, loss_function, optimizer)\n",
    "        test(model, test_loader, loss_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Матричные разложения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В матричных разложениях используется таблица юзеров-айтемов -- таблица, где по строкам находятся юзеры, по столбцам айтемы, на пересечениях оценка, которую поставил пользователь.\n",
    "\n",
    "Эта таблица представляется в виде произведения двух матриц, матрицы пользователей и матрицы айтемов\n",
    "\n",
    "![разложение](images/PQ.drawio.png)\n",
    "\n",
    "В каждом столбце матрицы пользователей живёт вектор, соответствующий этому пользователю, в матрице айтема, соответственно, вектор айтема. Чтобы получить предсказание оценки, надо их перемножить.\n",
    "\n",
    "Есть много разных способов находить матричные разложения, поскольку у нас тут pytorch, мы просто возьмём два `Embedding` слоя, перемножим, и скажем что это наша модель, которую обучим градиентным спуском\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.user_embeddings  = nn.Embedding(1000,  16)\n",
    "        self.movie_embeddings = nn.Embedding(10000, 16)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        movie_emb = self.user_embeddings(batch['user'])\n",
    "        user_emb = self.movie_embeddings(batch['movie'])\n",
    "        return (movie_emb * user_emb).sum(2)\n",
    "\n",
    "\n",
    "mf_model = MatrixFactorization().to(device)\n",
    "mf_loss = nn.MSELoss()\n",
    "mf_optimizer = torch.optim.SGD(mf_model.parameters(), lr=1)\n",
    "\n",
    "train(10, mf_model, mf_loss, mf_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фактически, если к этой моделе в сумму добавить общую константу и константу для кадого пользователя и айтема, мы получим Factorization Machine\n",
    "\n",
    "https://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf\n",
    "\n",
    "А это значит, что помимо эмбедингов с юзерами и айтемами мы можем легко добавить дополнительных параметров! (например тех, что у нас в таблице tags.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepFM это расширение обычной Factorization Machine для использования \n",
    "\n",
    "https://arxiv.org/pdf/1703.04247\n",
    "\n",
    "Идея состоит в том, чтобы расположить рядом с обычной Factorization Machine нейронную сеть, которая будет параллельно существовать с матричным разложением\n",
    "\n",
    "![DeepFM](images/DeepFM.png)\n",
    "\n",
    "До DeepFM уже были модели, которые предварительно обучали эмбединги на матричном разложении, а потом использовали их как входные векторы сети, но тут предлагается обучать их сразу совместно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.user_embeddings  = nn.Embedding(1000,  16)\n",
    "        self.movie_embeddings = nn.Embedding(10000, 16)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.deep_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.final_layer = nn.Linear(16*3, 1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        movie_emb = self.flatten(self.user_embeddings(batch['user']))\n",
    "        user_emb  = self.flatten(self.movie_embeddings(batch['movie']))\n",
    "\n",
    "        fm = movie_emb * user_emb\n",
    "\n",
    "        deep = torch.cat([movie_emb, user_emb], 1)\n",
    "        deep = self.deep_layers(deep)\n",
    "\n",
    "        v = torch.cat([fm, deep], 1)\n",
    "        v = self.final_layer(v)\n",
    "        # делаем сигмоиду на выходе и масштабируем к оценкам от 0 до 5\n",
    "        return torch.sigmoid(v) * 5\n",
    "\n",
    "\n",
    "deep_mf_model = DeepFM().to(device)\n",
    "deep_mf_loss = nn.MSELoss()\n",
    "deep_mf_optimizer = torch.optim.SGD(deep_mf_model.parameters(), lr=1e-1)\n",
    "\n",
    "train(5, deep_mf_model, deep_mf_loss, deep_mf_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть и более прокаченные версии машины факторизации на нейронках, например xDeepFM\n",
    "\n",
    "https://arxiv.org/pdf/1803.05170"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основное задание:\n",
    "1) Достичь меньше чем 0.8 значения MSELoss на этом датасете (5 баллов)\n",
    "2) МОЖНО ДЕЛАТЬ ТОЛЬКО ПОСЛЕ ТОГО КАК СДЕЛАНО ПЕРВОЕ ЗАДАНИЕ!  \n",
    "    Добавить в тренировочный датасет нового пользователя - себя и дать оценки минимум 20 фильмов, обучить модель с учётом этого пользователя и сделать для себя рекомендации. (5 баллов) (пожалуйста, не дописывайте себя в файлик, сделайте пользователя добавление в питоне)\n",
    "\n",
    "Дополнительные задания:\n",
    "1) Добавить в модель использование тегов из таблички `tags.csv` (5 дополнительных баллов)\n",
    "2) Добавить в модель использование дополнительных данных из источников `links.csv` (5 дополнительных баллов)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
