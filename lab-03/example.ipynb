{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лаб-3. Рекомендательные системы"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T21:25:50.853839Z",
     "start_time": "2024-12-05T21:25:50.850779Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Выбираем девайс\n",
    "device = \"cpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Device: {device}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве датасета будем использовать MovieLens\n",
    "\n",
    "https://grouplens.org/datasets/movielens/\n",
    "\n",
    "А именно, самый маленький вариант со 100 тыс. оценок\n",
    "\n",
    "https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T21:25:51.131205Z",
     "start_time": "2024-12-05T21:25:50.868091Z"
    }
   },
   "source": [
    "# Для загрузки датасета напишем свою реализацию класса Dataset\n",
    "class MovielensDataset(Dataset):\n",
    "    r\"\"\"seed должен быть одинаковым для обучающей и тренировочной выборки\"\"\"\n",
    "    def __init__(self, source, train=True, seed=1):\n",
    "        ratings      = pd.read_csv(rf\"{source}/ratings.csv\")\n",
    "        self.movies  = pd.read_csv(rf\"{source}/movies.csv\")\n",
    "\n",
    "        # Преобразовываем Id фильмов в индексы в таблице movies\n",
    "        x = self.movies.loc[:,['movieId']]\n",
    "        x['movieId'], x.index = x.index, x['movieId'].values\n",
    "        ratings['movieId'] = ratings['movieId'].map(x.to_dict()['movieId'])\n",
    "\n",
    "        # делим датасет 80% на 20%\n",
    "        train_data = ratings.sample(frac=0.8, random_state=seed)\n",
    "        test_data  = ratings.drop(train_data.index)\n",
    "\n",
    "        self.ratings = train_data if train else test_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.ratings.iloc[idx]\n",
    "        return {\n",
    "            \"user\": torch.LongTensor([sample['userId']]),\n",
    "            \"movie\": torch.LongTensor([sample['movieId']]),\n",
    "            \"rating\": torch.FloatTensor([sample['rating']])\n",
    "        }\n",
    "\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "sataset_source = r'./data'\n",
    "\n",
    "movielens_train = MovielensDataset(sataset_source, train=True)\n",
    "movielens_test  = MovielensDataset(sataset_source, train=False)\n",
    "\n",
    "train_loader = DataLoader(movielens_train, batch_size, True)\n",
    "test_loader = DataLoader(movielens_test, batch_size, True)"
   ],
   "outputs": [],
   "execution_count": 94
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T21:25:51.148122Z",
     "start_time": "2024-12-05T21:25:51.132157Z"
    }
   },
   "source": [
    "for batch in train_loader:\n",
    "    for k, v in batch.items():\n",
    "        print(k, v.shape)\n",
    "    break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user torch.Size([200, 1])\n",
      "movie torch.Size([200, 1])\n",
      "rating torch.Size([200, 1])\n"
     ]
    }
   ],
   "execution_count": 95
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T21:25:51.194922Z",
     "start_time": "2024-12-05T21:25:51.148748Z"
    }
   },
   "source": [
    "\n",
    "# Функции для обучения из прошлой лабы, с учётом юзеров и айтемов\n",
    "\n",
    "def train_iteration(model, data_loader, loss_function, optimizer):\n",
    "    model.train()\n",
    "    train_size = len(data_loader.dataset)\n",
    "    for idx, batch in enumerate(data_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        pred = model(batch)\n",
    "        loss = loss_function(pred, batch['rating'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if idx % 100 == 0:\n",
    "            loss, current = loss.item(), (idx + 1) * batch_size\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{train_size:>5d}]\")\n",
    "\n",
    "def test(model, data_loader, loss_function):\n",
    "    model.eval()\n",
    "    num_batches = len(data_loader)\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            pred = model(batch)\n",
    "            loss += loss_function(pred, batch['rating']).item()\n",
    "\n",
    "    loss /= num_batches\n",
    "    print(f\"Avg loss: {loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "def train(epochs, model, loss_function, optimizer):\n",
    "    for t in tqdm(range(epochs)):\n",
    "        print(f\"== Epoch {t + 1} ==\")\n",
    "        train_iteration(model, train_loader, loss_function, optimizer)\n",
    "        test(model, test_loader, loss_function)\n"
   ],
   "outputs": [],
   "execution_count": 96
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Матричные разложения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В матричных разложениях используется таблица юзеров-айтемов -- таблица, где по строкам находятся юзеры, по столбцам айтемы, на пересечениях оценка, которую поставил пользователь.\n",
    "\n",
    "Эта таблица представляется в виде произведения двух матриц, матрицы пользователей и матрицы айтемов\n",
    "\n",
    "![разложение](images/PQ.drawio.png)\n",
    "\n",
    "В каждом столбце матрицы пользователей живёт вектор, соответствующий этому пользователю, в матрице айтема, соответственно, вектор айтема. Чтобы получить предсказание оценки, надо их перемножить.\n",
    "\n",
    "Есть много разных способов находить матричные разложения, поскольку у нас тут pytorch, мы просто возьмём два `Embedding` слоя, перемножим, и скажем что это наша модель, которую обучим градиентным спуском\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T21:25:51.273420Z",
     "start_time": "2024-12-05T21:25:51.195735Z"
    }
   },
   "source": [
    "# class MatrixFactorization(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.user_embeddings  = nn.Embedding(1000,  16)\n",
    "#         self.movie_embeddings = nn.Embedding(10000, 16)\n",
    "# \n",
    "#     def forward(self, batch):\n",
    "#         movie_emb = self.user_embeddings(batch['user'])\n",
    "#         user_emb = self.movie_embeddings(batch['movie'])\n",
    "#         return (movie_emb * user_emb).sum(2)\n",
    "# \n",
    "# \n",
    "# mf_model = MatrixFactorization().to(device)\n",
    "# mf_loss = nn.MSELoss()\n",
    "# mf_optimizer = torch.optim.SGD(mf_model.parameters(), lr=1)\n",
    "# train(10, mf_model, mf_loss, mf_optimizer)"
   ],
   "outputs": [],
   "execution_count": 97
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фактически, если к этой моделе в сумму добавить общую константу и константу для кадого пользователя и айтема, мы получим Factorization Machine\n",
    "\n",
    "https://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf\n",
    "\n",
    "А это значит, что помимо эмбедингов с юзерами и айтемами мы можем легко добавить дополнительных параметров! (например тех, что у нас в таблице tags.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepFM это расширение обычной Factorization Machine для использования \n",
    "\n",
    "https://arxiv.org/pdf/1703.04247\n",
    "\n",
    "Идея состоит в том, чтобы расположить рядом с обычной Factorization Machine нейронную сеть, которая будет параллельно существовать с матричным разложением\n",
    "\n",
    "![DeepFM](images/DeepFM.png)\n",
    "\n",
    "До DeepFM уже были модели, которые предварительно обучали эмбединги на матричном разложении, а потом использовали их как входные векторы сети, но тут предлагается обучать их сразу совместно"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T21:26:19.616404Z",
     "start_time": "2024-12-05T21:25:51.274022Z"
    }
   },
   "source": [
    "class DeepFM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.user_embeddings  = nn.Embedding(1000,  16)\n",
    "        self.movie_embeddings = nn.Embedding(10000, 16)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.deep_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.final_layer = nn.Linear(16*3, 1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        movie_emb = self.flatten(self.user_embeddings(batch['user']))\n",
    "        user_emb  = self.flatten(self.movie_embeddings(batch['movie']))\n",
    "\n",
    "        fm = movie_emb * user_emb\n",
    "\n",
    "        deep = torch.cat([movie_emb, user_emb], 1)\n",
    "        deep = self.deep_layers(deep)\n",
    "\n",
    "        v = torch.cat([fm, deep], 1)\n",
    "        v = self.final_layer(v)\n",
    "        # делаем сигмоиду на выходе и масштабируем к оценкам от 0 до 5\n",
    "        return torch.sigmoid(v) * 5\n",
    "\n",
    "EPOCHS_COUNT = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "deep_mf_model = DeepFM().to(device)\n",
    "deep_mf_loss = nn.MSELoss()\n",
    "deep_mf_optimizer = torch.optim.SGD(deep_mf_model.parameters(), lr=1e-1)\n",
    "\n",
    "train(EPOCHS_COUNT, deep_mf_model, deep_mf_loss, deep_mf_optimizer)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Epoch 1 ==\n",
      "loss: 2.403590  [  200/80669]\n",
      "loss: 1.167551  [20200/80669]\n",
      "loss: 1.126825  [40200/80669]\n",
      "loss: 1.058344  [60200/80669]\n",
      "loss: 0.922309  [80200/80669]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:11<00:47, 11.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.065521 \n",
      "\n",
      "== Epoch 2 ==\n",
      "loss: 1.185928  [  200/80669]\n",
      "loss: 1.109425  [20200/80669]\n",
      "loss: 1.121564  [40200/80669]\n",
      "loss: 1.107710  [60200/80669]\n",
      "loss: 1.064630  [80200/80669]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:15<00:21,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 1.035829 \n",
      "\n",
      "== Epoch 3 ==\n",
      "loss: 1.064283  [  200/80669]\n",
      "loss: 0.960729  [20200/80669]\n",
      "loss: 1.109316  [40200/80669]\n",
      "loss: 0.933342  [60200/80669]\n",
      "loss: 1.058448  [80200/80669]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:19<00:11,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.986334 \n",
      "\n",
      "== Epoch 4 ==\n",
      "loss: 0.765155  [  200/80669]\n",
      "loss: 0.991770  [20200/80669]\n",
      "loss: 0.989166  [40200/80669]\n",
      "loss: 1.076785  [60200/80669]\n",
      "loss: 1.039737  [80200/80669]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:24<00:05,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.995057 \n",
      "\n",
      "== Epoch 5 ==\n",
      "loss: 0.936180  [  200/80669]\n",
      "loss: 1.073245  [20200/80669]\n",
      "loss: 1.013268  [40200/80669]\n",
      "loss: 0.924590  [60200/80669]\n",
      "loss: 0.908451  [80200/80669]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:28<00:00,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.994050 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть и более прокаченные версии машины факторизации на нейронках, например xDeepFM\n",
    "\n",
    "https://arxiv.org/pdf/1803.05170"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основное задание:\n",
    "1) Достичь меньше чем 0.8 значения MSELoss на этом датасете (5 баллов)\n",
    "2) МОЖНО ДЕЛАТЬ ТОЛЬКО ПОСЛЕ ТОГО КАК СДЕЛАНО ПЕРВОЕ ЗАДАНИЕ!  \n",
    "    Добавить в тренировочный датасет нового пользователя - себя и дать оценки минимум 20 фильмов, обучить модель с учётом этого пользователя и сделать для себя рекомендации. (5 баллов) (пожалуйста, не дописывайте себя в файлик, сделайте пользователя добавление в питоне)\n",
    "\n",
    "Дополнительные задания:\n",
    "1) Добавить в модель использование тегов из таблички `tags.csv` (5 дополнительных баллов)\n",
    "2) Добавить в модель использование дополнительных данных из источников `links.csv` (5 дополнительных баллов)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
